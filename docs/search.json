[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emre Çakmak Progress Journal",
    "section": "",
    "text": "This progress journal covers Emre Çakmak’s work during their term at BDA 503 Fall 2022.\nEach section is an assignment or an individual work."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "1  BDA-503 Assigment 1",
    "section": "",
    "text": "Hi dear reader,\nI’m Emre Çakmak from Istanbul/Turkey. I graduated from my bachelor at Istanbul Technical University, Industrial Engineering Department in 2018.\nMy current role is Data Scientist at E-commerce Department in LC Waikiki which is a Istanbul based global fashion retailer driving operations on more than 50 countries. I had different positions like Data Analyst, Business Intelligence Specialist in different companies during past 4 years. Especially in last 1 year, I dedicated to improve myself for application of ML Technics due to enrich customer&item based data. So, I’m a part of BDA Graduate Program in MEF University to wide my knowledge in audience management and marketing applications by the help of real-life use cases.\nHere is my LinkedIn Profile"
  },
  {
    "objectID": "assignment1.html#rstudio-global-2022-conference---quarto-for-the-curious",
    "href": "assignment1.html#rstudio-global-2022-conference---quarto-for-the-curious",
    "title": "1  BDA-503 Assigment 1",
    "section": "1.2 RStudio Global 2022 Conference - Quarto for the Curious",
    "text": "1.2 RStudio Global 2022 Conference - Quarto for the Curious\n\nWhat’s Quarto according to Tom Mock\nIn this paragraph, I aim to give you some main differences between Quarto, the brand new documentation system which has been released April 2022, and RMarkdown being used for almost a decade.\n\nTom Mock says Quarto is Open source scientific and technical publishing system. Also he added that Quarto is the next generation of RMarkdown.\n\nHere is some differences between them:\n\n1.2.1 Preprocessing\n\n\n\n\n\n\n\n(a) RMarkdown\n\n\n\n\n\n\n\n(b) Quarto\n\n\n\n\nFigure 1.1: RMarkdown vs Quarto Preprocessing Diagram\n\n\nAltough it seems like they have almost same workflow behind the scenes; Quarto doesn’t need to have R in the system to use it. It means that you can use Quarto in a fresh computer but Rmarkdown needs to have R in the system.\n\n\n1.2.2 Language Support\nThe main purpose of releasing Quarto is improving the communication between data science communities whatever their language is. Because of this Quarto supports other languages as engine.\n\n\n\nFigure 1.2: Jupyter as Quarto Engine\n\n\nThis availability in Quarto and not limiting with R allows people to collaborate as Python developer with others. Tom Mock figured this situation out like\n\nQuarto: Comfortable baking in your own kitchen\nRMarkdown: Uncomfortable baking in corporate kitchen"
  },
  {
    "objectID": "assignment1.html#r-posts",
    "href": "assignment1.html#r-posts",
    "title": "1  BDA-503 Assigment 1",
    "section": "1.3 R Posts",
    "text": "1.3 R Posts\nThis section includes 3 different R Programming use case\n\n1.3.1 Web Scraping with R\nIt’s very known fact that people have some struggle to access to a clean dataset. In these cases, we need to be a little bit creative to create our own dataset. And one way of the creating a new dataset is web scraping.\nIn this paragraph, I want to introduce how to scrape a web page by the help of R packages. The most common 2 packages are:\n\n{rvest}\n{RSelenium}\n\nNote that: Some websites have strict policies against scraping. Be careful!\nStep by step scraping of public IMDB Dataset\nStep 1: Install Package\n\n## Before you start, you need to execute once the code below.\n##install.packages(\"rvest\")\n\nStep 2: Call the library and use html functions\n\n## call the rvest library for required functions\nlibrary(rvest)\n\n## define the website link you want to scrape\nlink = \"https://www.imdb.com/search/title/?title_type=feature&num_votes=30000,&genres=comedy\"\n\n## send a http get request to the link above and store it in a variable\npage = read_html(link)\n\n## filter and grab all elements in same class\ntitles = page %>% html_nodes(\".lister-item-header a\") %>% html_text()\n\n## preview the titles\ntitles[1:10]\n\n [1] \"Bullet Train\"                      \"Hocus Pocus\"                      \n [3] \"Hocus Pocus 2\"                     \"Everything Everywhere All at Once\"\n [5] \"Thor: Love and Thunder\"            \"Beetle Juice\"                     \n [7] \"The Rocky Horror Picture Show\"     \"The Lost City\"                    \n [9] \"The Goonies\"                       \"Trick 'r Treat\"                   \n\n\nStep 3: Create other variables\n\n## apply same procedure to other variables\nyear= page %>% html_nodes(\".text-muted.unbold\") %>% html_text()\nrating = page %>% html_nodes(\".ratings-imdb-rating strong\") %>% html_text()\n\n\n## preview variables\nyear[1:10]\n\n [1] \"(2022)\" \"(1993)\" \"(2022)\" \"(2022)\" \"(2022)\" \"(1988)\" \"(1975)\" \"(2022)\"\n [9] \"(1985)\" \"(2007)\"\n\nrating[1:10]\n\n [1] \"7.3\" \"6.9\" \"6.0\" \"8.1\" \"6.4\" \"7.5\" \"7.4\" \"6.1\" \"7.7\" \"6.7\"\n\n\nStep 4: Create data frame\n\n## create a dataset\nmovies = data.frame(titles, year, rating, stringsAsFactors = FALSE)\nmovies[1:10,]\n\n                              titles   year rating\n1                       Bullet Train (2022)    7.3\n2                        Hocus Pocus (1993)    6.9\n3                      Hocus Pocus 2 (2022)    6.0\n4  Everything Everywhere All at Once (2022)    8.1\n5             Thor: Love and Thunder (2022)    6.4\n6                       Beetle Juice (1988)    7.5\n7      The Rocky Horror Picture Show (1975)    7.4\n8                      The Lost City (2022)    6.1\n9                        The Goonies (1985)    7.7\n10                    Trick 'r Treat (2007)    6.7\n\n\nReferences of web scraping with R:\n\nScraperapi\nScrapingbee\nAppsilon\n\n\n\n1.3.2 Simple Aggregations on Dataset\nThis part provides some basic aggregations and data manipulation methods in R via {dplyr} package.\nWithout leaving the concept in previous part, we can assume that we created our own dataset. So, what’s next?\nThe process of extracting insightful information from datasets starts from understanding the data structure and manipulating them. R provides a package just for this: {dplyr}\nStep by step aggregation & filtering & summarizing dataset\nStep 1: Install Package\n\n## Before you start, you need to execute once the code below.\n##install.packages(\"dplyr\")\n\nStep 2: Call the library\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nStep 3: Select subset of data in different aspects\n\n## selecting specific columns\nselect(movies, titles, year)[1:10,]\n\n                              titles   year\n1                       Bullet Train (2022)\n2                        Hocus Pocus (1993)\n3                      Hocus Pocus 2 (2022)\n4  Everything Everywhere All at Once (2022)\n5             Thor: Love and Thunder (2022)\n6                       Beetle Juice (1988)\n7      The Rocky Horror Picture Show (1975)\n8                      The Lost City (2022)\n9                        The Goonies (1985)\n10                    Trick 'r Treat (2007)\n\n## filter data according to specific condition\nfilter(movies, rating > 8)\n\n                             titles       year rating\n1 Everything Everywhere All at Once     (2022)    8.1\n2           The Wolf of Wall Street     (2013)    8.2\n3                Back to the Future     (1985)    8.5\n4                Young Frankenstein     (1974)    8.0\n5                              Coco (I) (2017)    8.4\n\n## sort rows\narrange(movies, desc(titles))[1:10,]\n\n                                    titles       year rating\n1                       Young Frankenstein     (1974)    8.0\n2                                     Tusk (I) (2014)    5.3\n3                           Trick 'r Treat     (2007)    6.7\n4                   Thor: Love and Thunder     (2022)    6.4\n5                  The Wolf of Wall Street     (2013)    8.2\n6  The Unbearable Weight of Massive Talent     (2022)    7.0\n7                        The Suicide Squad     (2021)    7.2\n8            The Rocky Horror Picture Show     (1975)    7.4\n9                            The Lost City     (2022)    6.1\n10                           The Lost Boys     (1987)    7.2\n\n## select top n rows\ntop_n(movies, 3, titles)\n\n              titles       year rating\n1     Trick 'r Treat     (2007)    6.7\n2 Young Frankenstein     (1974)    8.0\n3               Tusk (I) (2014)    5.3\n\n\nStep 4: Summarize Dataset\n\n## convert rating columns as numeric and calculate the average\nsummarise(movies, average_rating = mean(as.numeric(rating)))\n\n  average_rating\n1          6.976\n\n## group by and summarize\ngrouped_data = group_by(movies, year)\nsummarise(grouped_data, average_rating = mean(as.numeric(rating)))[1:5,]\n\n# A tibble: 5 × 2\n  year   average_rating\n  <chr>           <dbl>\n1 (1974)            8  \n2 (1975)            7.4\n3 (1984)            7.8\n4 (1985)            8.1\n5 (1986)            7.1\n\n\nStep 5: %>% Operator\nThis operator takes the object from the left and gives it as the first argument to the function on the right. It makes your code more readable.\n\n## same grouping and summarizing operation at step4 \n\nmovies %>%\n  group_by(year) %>%\n  summarise(average_rating = mean(as.numeric(rating)))%>%\n  top_n(5, desc(average_rating))\n\n# A tibble: 5 × 2\n  year       average_rating\n  <chr>               <dbl>\n1 (2002)                5.2\n2 (2009)                5.4\n3 (2020)                5.2\n4 (I) (2014)            5.3\n5 (I) (2022)            5.1\n\n\nReference of aggregations with R:\n\ncourses.cs.ut.ee\n\n\n\n1.3.3 Visualization with R\nStep 1: Install Package\n\n## Before you start, you need to execute once the code below.\n##install.packages(\"ggplot2\")\n\nStep 2: Call the library\n\nlibrary(ggplot2)\n\nStep 3: Histogram with ggplot2\n\n## convert rating field as numeric and keep in original dataset\nmovies$rating=as.numeric(rating)\n\n## histogram for ratings\nhist(movies$rating,col='steelblue',main='Rating Histogram',\n     xlab='Ratings')\n\n\n\n\nStep 4: Pie Chart with ggplot2\n\n## Set a new flag in dataset\nmovies=movies %>% mutate(rating_flag = case_when(rating>=8~ \"Higher 8\", TRUE ~ \"Lower 8\"))\n\n## creating a new table for better visualization\ncount_movies=movies %>% count(rating_flag)\n\n## pie chart according to rating of movies\nggplot(count_movies, aes(x = \"\", y = n, fill = rating_flag)) +\n  geom_col(color = \"black\") +\n  geom_label(aes(label = n),\n             position = position_stack(vjust = 0.5),\n             show.legend = FALSE) +\n  coord_polar(theta = \"y\")\n\n\n\n\nReferences of visualization with R:\n\nr-charts\nkdnuggets"
  },
  {
    "objectID": "inclass1.html",
    "href": "inclass1.html",
    "title": "2  Inclass Exercise-1",
    "section": "",
    "text": "This exercise has been prepared for understanding {dplyr} package usage for functional EDA. Main data set in this exercise will be planes data set derived from FAA.\nFirst of all we have to install our packages.\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"nycflights13\")\n\nThen we are calling our libraries.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nLet’s check first 10 rows of the data set. Fields and their meanings are:\n\ntailnum: Tail number.\nyear: Year manufactured.\ntype: Type of plane.\nmanufacturer: Manufacturer of the aircraft.\nmodel: Model of the aircraft.\nengines: Number of engines\nseats: Number of seats\nspeed: Average cruising speed in mph.\nengine: Type of engine.\n\n\nplanes %>% \n  slice(1:10)\n\n# A tibble: 10 × 9\n   tailnum  year type                   manuf…¹ model engines seats speed engine\n   <chr>   <int> <chr>                  <chr>   <chr>   <int> <int> <int> <chr> \n 1 N10156   2004 Fixed wing multi engi… EMBRAER EMB-…       2    55    NA Turbo…\n 2 N102UW   1998 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n 3 N103US   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n 4 N104UW   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n 5 N10575   2002 Fixed wing multi engi… EMBRAER EMB-…       2    55    NA Turbo…\n 6 N105UW   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n 7 N107US   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n 8 N108UW   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n 9 N109UW   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n10 N110UW   1999 Fixed wing multi engi… AIRBUS… A320…       2   182    NA Turbo…\n# … with abbreviated variable name ¹​manufacturer"
  },
  {
    "objectID": "inclass1.html#exercise-1",
    "href": "inclass1.html#exercise-1",
    "title": "2  Inclass Exercise-1",
    "section": "2.1 EXERCISE 1",
    "text": "2.1 EXERCISE 1\nNow, how many aircraft does exists for each manufacturing company? Let’s calculate..\n\nplanes %>% \n  group_by(manufacturer) %>% \n  summarise(aircraft_count = n()) %>% \n  arrange(desc(aircraft_count)) %>% \n  print(n=Inf)\n\n# A tibble: 35 × 2\n   manufacturer                  aircraft_count\n   <chr>                                  <int>\n 1 BOEING                                  1630\n 2 AIRBUS INDUSTRIE                         400\n 3 BOMBARDIER INC                           368\n 4 AIRBUS                                   336\n 5 EMBRAER                                  299\n 6 MCDONNELL DOUGLAS                        120\n 7 MCDONNELL DOUGLAS AIRCRAFT CO            103\n 8 MCDONNELL DOUGLAS CORPORATION             14\n 9 CANADAIR                                   9\n10 CESSNA                                     9\n11 PIPER                                      5\n12 AMERICAN AIRCRAFT INC                      2\n13 BEECH                                      2\n14 BELL                                       2\n15 GULFSTREAM AEROSPACE                       2\n16 STEWART MACO                               2\n17 AGUSTA SPA                                 1\n18 AVIAT AIRCRAFT INC                         1\n19 AVIONS MARCEL DASSAULT                     1\n20 BARKER JACK L                              1\n21 CANADAIR LTD                               1\n22 CIRRUS DESIGN CORP                         1\n23 DEHAVILLAND                                1\n24 DOUGLAS                                    1\n25 FRIEDEMANN JON                             1\n26 HURLEY JAMES LARRY                         1\n27 JOHN G HESS                                1\n28 KILDALL GARY                               1\n29 LAMBERT RICHARD                            1\n30 LEARJET INC                                1\n31 LEBLANC GLENN T                            1\n32 MARZ BARRY                                 1\n33 PAIR MIKE E                                1\n34 ROBINSON HELICOPTER CO                     1\n35 SIKORSKY                                   1\n\n\nIt seems like there is a conflict in manufacturer names. Some of them represent the same company but in different names like Airbus and Airbus Industrie.\nWe need to clean and rewrite these names. Then we can apply same process again.\n\nplanes =\nplanes %>% \n  mutate(manufacturer = gsub(\"AIRBUS INDUSTRIE\", \"AIRBUS\", manufacturer), manufacturer=gsub(\" AIRCRAFT CO| CORPORATION\", \"\", manufacturer))\n\nThe last version of distribution of air crafts according to their manufacturer is here.\n\nplanes %>% \n  group_by(manufacturer) %>% \n  summarise(aircraft_count = n()) %>% \n  arrange(desc(aircraft_count)) %>% \n  mutate(aircraft_count_distrubiton=round(aircraft_count/sum(aircraft_count),2)) %>% \n  print(n=Inf)\n\n# A tibble: 32 × 3\n   manufacturer           aircraft_count aircraft_count_distrubiton\n   <chr>                           <int>                      <dbl>\n 1 BOEING                           1630                       0.49\n 2 AIRBUS                            736                       0.22\n 3 BOMBARDIER INC                    368                       0.11\n 4 EMBRAER                           299                       0.09\n 5 MCDONNELL DOUGLAS                 237                       0.07\n 6 CANADAIR                            9                       0   \n 7 CESSNA                              9                       0   \n 8 PIPER                               5                       0   \n 9 AMERICAN AIRCRAFT INC               2                       0   \n10 BEECH                               2                       0   \n11 BELL                                2                       0   \n12 GULFSTREAM AEROSPACE                2                       0   \n13 STEWART MACO                        2                       0   \n14 AGUSTA SPA                          1                       0   \n15 AVIAT AIRCRAFT INC                  1                       0   \n16 AVIONS MARCEL DASSAULT              1                       0   \n17 BARKER JACK L                       1                       0   \n18 CANADAIR LTD                        1                       0   \n19 CIRRUS DESIGN CORP                  1                       0   \n20 DEHAVILLAND                         1                       0   \n21 DOUGLAS                             1                       0   \n22 FRIEDEMANN JON                      1                       0   \n23 HURLEY JAMES LARRY                  1                       0   \n24 JOHN G HESS                         1                       0   \n25 KILDALL GARY                        1                       0   \n26 LAMBERT RICHARD                     1                       0   \n27 LEARJET INC                         1                       0   \n28 LEBLANC GLENN T                     1                       0   \n29 MARZ BARRY                          1                       0   \n30 PAIR MIKE E                         1                       0   \n31 ROBINSON HELICOPTER CO              1                       0   \n32 SIKORSKY                            1                       0"
  },
  {
    "objectID": "inclass1.html#exercise-2",
    "href": "inclass1.html#exercise-2",
    "title": "2  Inclass Exercise-1",
    "section": "2.2 EXERCISE 2",
    "text": "2.2 EXERCISE 2\nLet’s check the difference on aircraft capacities year by year\nFirst, get only air crafts which have more than 50 seats. Then clear the data by filtering rows which have no information in Year column.\n\nplanes %>% \n  filter(seats>50,!is.na(year))%>%\n  group_by(year) %>% \n  summarise(seat_avg = round(mean(seats),2)) %>% \n  arrange(year) %>% \n  print(n=Inf)\n\n# A tibble: 38 × 2\n    year seat_avg\n   <int>    <dbl>\n 1  1956     102 \n 2  1965     149 \n 3  1975     139 \n 4  1976     139 \n 5  1977     139 \n 6  1978     139 \n 7  1979     139 \n 8  1980     139 \n 9  1984     178 \n10  1985     174.\n11  1986     196.\n12  1987     181.\n13  1988     190.\n14  1989     163.\n15  1990     179.\n16  1991     181.\n17  1992     195.\n18  1993     198.\n19  1994     178.\n20  1995     187.\n21  1996     170.\n22  1997     179.\n23  1998     169.\n24  1999     167.\n25  2000     163.\n26  2001     152.\n27  2002     132.\n28  2003     106.\n29  2004     116.\n30  2005     117.\n31  2006     141.\n32  2007     140.\n33  2008     147.\n34  2009     194.\n35  2010     164.\n36  2011     214.\n37  2012     207.\n38  2013     206.\n\n\nLet’s check the biggest air craft in our database with it’s tailnumber.\n\nplanes %>%\n  arrange(desc(seats)) %>% \n  slice(1) %>%\n  select(tailnum, manufacturer, model, year, seats)\n\n# A tibble: 1 × 5\n  tailnum manufacturer model    year seats\n  <chr>   <chr>        <chr>   <int> <int>\n1 N670US  BOEING       747-451  1990   450\n\n\nExciting..Here is some information about the biggest airplane’s history\nTHANKS FOR READING"
  },
  {
    "objectID": "Python_WebScraping.html",
    "href": "Python_WebScraping.html",
    "title": "3  Web Scraping and Clustering in Python",
    "section": "",
    "text": "We are importing the required libraries. Especially selenium for scraping, pandas for dataframes and sklearn for clustering processes.\nStep by step;\n\nWe are defining the scraping URL\nWe need to download chromedriver.exe to connect Google Chrome\nThis web page has infinitive scroll. So, we need to set scrolling depth which should be to the bottom.\nThen, we need to catch necessary fields, item name and price.\nIn this project, item names includes the coordinates of the NFT and it will give us enough information about the location.\nWe will use this coordinates to calculate distance and region of the NFT\nDistance/Price rate is a spectacular field to determine best affordable NFT in terms of it’s location.\nCluster number has been predefined in this example but it depends on user’s own decision according to elbow method.\nAfter clustering, data has been grouped by cluster numbers and calculated their mean and std.\nNFTs have been filtered by their distance/price rate if they are lower then their cluster’s “mean-1.5*std”\n\n\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport socket\n \nimport io\nimport shutil\nimport re\nimport urllib.request\nimport os\nimport errno\n\nimport urllib.request\nimport xlsxwriter\nimport time\n\nimport pandas\nimport numpy\n\nimport matplotlib.pyplot as plt\nfrom kneed import KneeLocator\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\n \ndriver = webdriver.Chrome(r\"C:\\Users\\EMRE\\Documents\\GitHub\\mef06-cakmakem\\ScrapingFiles\\chromedriver.exe\")\ndriver.get(\"https://www.jpg.store/collection/pavia?minPrice=300000000&maxPrice=1000000000\")\n\nSCROLL_PAUSE_TIME = 0.8\n\n# Get scroll height\nlast_height = driver.execute_script(\"return document.body.scrollHeight\")\na=0\nn=5\nwhile a<n:\n    # Scroll down to bottom\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n    # Wait to load page\n    time.sleep(SCROLL_PAUSE_TIME)\n\n    # Calculate new scroll height and compare with last scroll height\n    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n    if new_height == last_height:\n        break\n    last_height = new_height\n    a+=1\n\n\nitemset = []\npriceset=[]\ncontent = driver.page_source\nsoup = BeautifulSoup(content)\nfor a in soup.find_all('div', attrs = {'class', 'styles_itemsGrid__J7c4P grid'}):\n    children = a.findChildren(\"span\", recursive=False)\n    for child in children:\n        sublings = child.find_all('div', attrs = {'class', 'NFTMarketplaceCard_nftMarketplaceCardContent__I5fdd'})\n        pricesclass = child.find_all('div', attrs = {'class', 'NFTMarketplaceCard_nftMarketplaceCardPriceContainer__a3CBp'})\n        for items in sublings:\n            a=items.findChildren(\"h4\",recursive=False)\n            #.find_all('h4', attrs = {'id', 'asset-title'})\n            #for i in a.contents:\n            itemset.append(a[0])\n        for price in pricesclass:\n            b=price.findChildren(\"div\", recursive=False)\n            for c in b:\n                d=c.findChildren(\"span\",recursive=False)\n                priceset.append(d[0])\n\ndf = pd.DataFrame(itemset, columns=[\"names\"])\ndf_p = pd.DataFrame(priceset, columns=['price'])\n\ndf_final =  pd.concat([df,df_p],axis=1)\n\n## df_final.to_excel('20102022_2_nft.xlsx', encoding='utf-16',engine='xlsxwriter')\n\n\ndf_final[['name','space','X','Y']]=df_final.names.str.split(' ',3,expand=True)\ndf_final=df_final[['X','Y','price']]\ndf_final=df_final[(df_final['X']!='-')&(df_final['Y']!='-')&(df_final['X']!='')&(df_final['Y']!='')&(df_final['X']!=' ')&(df_final['Y']!=' ')]\n\ndf_final['X']=pd.to_numeric(df_final['X'],downcast='integer')\ndf_final['Y']=pd.to_numeric(df_final['Y'],downcast='integer')\n\ndf_final['distance_to_origin']=(abs((df_final['X'])**2 + (df_final['Y']**2)))**0.5\n\nkmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\n# # A list holds the SSE values for each k\nsse = []\nfor k in range(1, 30):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(df_final[['X','Y','distance_to_origin']])\n    sse.append(kmeans.inertia_)\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 30), sse)\nplt.xticks(range(1, 30))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\n\n\n##print('specify number of clusters')\n##k=int(input())\n\nk=10\nkmeans = KMeans(n_clusters=k, **kmeans_kwargs)\ndf_final['cluster']=kmeans.fit_predict(df_final[['X','Y','distance_to_origin']])\ndf_final['cluster'] = df_final['cluster'].astype('category')\n\n\nresults = df_final.groupby('cluster').agg({'price':['mean','std']})\nresults=results.reset_index()\nresults.columns=['cluster','mean','std']\ndf_final=df_final.merge(results,left_on='cluster',right_on='cluster')\ndf_final['low_limit']=df_final['mean']-1.5*df_final['std']\ndf_final['d_p_rate'] = df_final['distance_to_origin']/df_final['price'].astype(float)\n\nresults_dp = df_final.groupby('cluster').agg({'d_p_rate':['mean','std']})\nresults_dp=results_dp.reset_index()\nresults_dp.columns=['cluster','mean_dp','std_dp']\ndf_final=df_final.merge(results_dp,left_on='cluster',right_on='cluster')\n\ndf_final['low_limit_dp']=df_final['mean_dp']-1.5*df_final['std_dp']\n\nfinalists=df_final[(df_final['price'].astype(float)<df_final['low_limit'])&(df_final['d_p_rate']<df_final['mean_dp'])]\n\nfinalists[1:10]\n\nC:\\Users\\EMRE\\AppData\\Local\\Temp\\ipykernel_10644\\1984115952.py:27: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n  driver = webdriver.Chrome(r\"C:\\Users\\EMRE\\Documents\\GitHub\\mef06-cakmakem\\ScrapingFiles\\chromedriver.exe\")\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      X\n      Y\n      price\n      distance_to_origin\n      cluster\n      mean\n      std\n      low_limit\n      d_p_rate\n      mean_dp\n      std_dp\n      low_limit_dp\n    \n  \n  \n    \n      4\n      -176\n      124\n      300\n      138.506318\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.461688\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      5\n      -204\n      60\n      300\n      142.548237\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.475161\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      11\n      -238\n      131\n      320\n      90.934042\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.284169\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      12\n      -194\n      74\n      324\n      149.746452\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.462180\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      13\n      -198\n      84\n      333\n      138.838035\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.416931\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      14\n      -197\n      82\n      333\n      141.431962\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.424721\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      15\n      -185\n      129\n      333\n      121.119775\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.363723\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      17\n      -196\n      46\n      344\n      158.126532\n      6\n      1.048475e+55\n      49.327762\n      1.048475e+55\n      0.459670\n      0.510455\n      0.118085\n      0.333327\n    \n    \n      25\n      -224\n      -11\n      300\n      123.446345\n      5\n      7.968328e+72\n      51.917659\n      7.968328e+72\n      0.411488\n      0.517249\n      0.129786\n      0.322569"
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "4  BDA-503 Shiny Assignment",
    "section": "",
    "text": "4.0.2 shinyapps.io\nForeign Students Shiny App\n\n\n4.0.3 Command for local running\nshiny::runGitHub(“/pjournal/mef06-cakmakem”,subdir=“emre_cakmak_assigment/”, ref = “gh-pages”)"
  }
]